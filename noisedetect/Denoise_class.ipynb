{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHXIgcrGVzG8",
        "outputId": "b9e90105-5504-4a27-c9b1-5e92def7b7b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.pyplot._IonContext at 0x1345cf12948>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "from PIL import Image\n",
        "import re\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "plt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jawA44PlcK4b"
      },
      "source": [
        "BOTAR AQUI SUA PASTA DO NOTEBOOK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u3FVa380cKR0"
      },
      "outputs": [],
      "source": [
        "# pasta_do_notebook = '/content/drive/MyDrive/Blur_CLas'\n",
        "pasta_do_notebook = '.'\n",
        "os.chdir(pasta_do_notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "O37XW_9Mbt9e",
        "outputId": "1f528eae-77d4-4c09-cb60-f7a6b05eeb3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\daniel_moreira\\\\reconhecimento_de_padroes\\\\reconhecimento_de_padroes\\\\noisedetect'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to find reference and noisy images in a folder\n",
        "def find_images_in_folder(folder):\n",
        "    reference_image = None\n",
        "    noisy_image = None\n",
        "\n",
        "    # Iterate through files in the folder\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.lower().endswith(\".jpg\"):\n",
        "            file_path = os.path.join(folder, filename)\n",
        "\n",
        "            # Check if the image contains \"Reference\" or \"Noisy\"\n",
        "            if \"reference\" in filename.lower():\n",
        "                reference_image = cv.imread(file_path)\n",
        "            elif \"noisy\" in filename.lower():\n",
        "                noisy_image = cv.imread(file_path)\n",
        "\n",
        "            # Break if both images are found\n",
        "            if reference_image is not None and noisy_image is not None:\n",
        "                break\n",
        "\n",
        "    return reference_image, noisy_image\n",
        "\n",
        "# Main function to process folders\n",
        "def read_renoir(root_folders, num_images=0):\n",
        "    print('Reading RENOIR...')\n",
        "    targets, labels = [], []\n",
        "    for root_folder in os.listdir(root_folders):\n",
        "        root_folder_path = os.path.join(root_folders, root_folder)\n",
        "        print('Reading folder: ', root_folder)\n",
        "        for foldername in os.listdir(root_folder_path):\n",
        "            folder_path = os.path.join(root_folder_path, foldername)\n",
        "            if os.path.isdir(folder_path):\n",
        "                reference_image, noisy_image = find_images_in_folder(folder_path)\n",
        "                \n",
        "                targets.append(noisy_image)\n",
        "                labels.append(reference_image)\n",
        "                if num_images > 0 and len(targets) % num_images == 0:\n",
        "                    break\n",
        "    print('Reading complete.')\n",
        "    y = np.ones(len(targets))\n",
        "    y = np.concatenate((y, np.zeros(len(labels))))\n",
        "    targets = np.concatenate((targets, labels))\n",
        "    return targets, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cjTzYf6UWrBW"
      },
      "outputs": [],
      "source": [
        "train_path = 'datasets/train'\n",
        "val_path = 'datasets/val'\n",
        "test_path = 'datasets/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading RENOIR...\n",
            "Reading folder:  Mi3_Aligned\n",
            "Reading folder:  S90_Aligned\n",
            "Reading folder:  T3i_Aligned\n",
            "Reading complete.\n",
            "Reading RENOIR...\n",
            "Reading folder:  Mi3_Aligned\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading folder:  S90_Aligned\n",
            "Reading folder:  T3i_Aligned\n",
            "Reading complete.\n",
            "Reading RENOIR...\n",
            "Reading folder:  Mi3_Aligned\n",
            "Reading folder:  S90_Aligned\n",
            "Reading folder:  T3i_Aligned\n",
            "Reading complete.\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = read_renoir(train_path, num_images=0)\n",
        "X_val, y_val = read_renoir(val_path, num_images=0)\n",
        "X_test, y_test = read_renoir(test_path, num_images=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYaCKO2PiHsf",
        "outputId": "50190797-54fb-4de2-cb6a-dbac3e7b5952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144\n",
            "48\n",
            "48\n",
            "[array([[[ 22,  64,  69],\n",
            "         [ 17,  61,  68],\n",
            "         [ 18,  63,  74],\n",
            "         ...,\n",
            "         [ 19,  37,  38],\n",
            "         [ 14,  38,  36],\n",
            "         [ 18,  48,  43]],\n",
            "\n",
            "        [[ 29,  65,  71],\n",
            "         [ 23,  60,  68],\n",
            "         [ 23,  61,  73],\n",
            "         ...,\n",
            "         [ 14,  28,  34],\n",
            "         [ 15,  36,  37],\n",
            "         [ 12,  36,  34]],\n",
            "\n",
            "        [[ 36,  59,  67],\n",
            "         [ 30,  55,  65],\n",
            "         [ 32,  58,  70],\n",
            "         ...,\n",
            "         [ 16,  25,  39],\n",
            "         [ 22,  32,  42],\n",
            "         [ 21,  33,  39]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 34,  51,  77],\n",
            "         [ 34,  55,  76],\n",
            "         [ 31,  59,  66],\n",
            "         ...,\n",
            "         [ 54,  92, 127],\n",
            "         [ 64, 100, 146],\n",
            "         [ 59,  96, 148]],\n",
            "\n",
            "        [[ 36,  65,  86],\n",
            "         [ 27,  61,  74],\n",
            "         [ 19,  59,  58],\n",
            "         ...,\n",
            "         [ 57, 105, 129],\n",
            "         [ 60, 111, 144],\n",
            "         [ 64, 116, 153]],\n",
            "\n",
            "        [[ 40,  76,  94],\n",
            "         [ 28,  67,  76],\n",
            "         [ 21,  67,  61],\n",
            "         ...,\n",
            "         [ 54, 107, 127],\n",
            "         [ 53, 110, 136],\n",
            "         [ 52, 112, 142]]], dtype=uint8)\n",
            " array([[[ 64,  71,  86],\n",
            "         [ 74,  64,  81],\n",
            "         [101,  56,  82],\n",
            "         ...,\n",
            "         [  3,  12,  25],\n",
            "         [  1,  10,  19],\n",
            "         [  5,  14,  18]],\n",
            "\n",
            "        [[ 64,  78,  90],\n",
            "         [ 70,  72,  83],\n",
            "         [ 79,  54,  68],\n",
            "         ...,\n",
            "         [  1,  13,  25],\n",
            "         [  4,  12,  25],\n",
            "         [  6,  15,  25]],\n",
            "\n",
            "        [[ 58,  89,  92],\n",
            "         [ 57,  83,  83],\n",
            "         [ 48,  64,  57],\n",
            "         ...,\n",
            "         [  2,  14,  26],\n",
            "         [  5,  14,  34],\n",
            "         [  6,  14,  37]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 83, 142, 161],\n",
            "         [ 83, 138, 159],\n",
            "         [ 98, 142, 166],\n",
            "         ...,\n",
            "         [157, 179, 250],\n",
            "         [151, 160, 227],\n",
            "         [193, 195, 255]],\n",
            "\n",
            "        [[ 69, 128, 144],\n",
            "         [ 71, 128, 143],\n",
            "         [ 74, 131, 140],\n",
            "         ...,\n",
            "         [167, 190, 255],\n",
            "         [152, 159, 239],\n",
            "         [179, 177, 255]],\n",
            "\n",
            "        [[ 76, 133, 148],\n",
            "         [ 77, 138, 148],\n",
            "         [ 65, 129, 130],\n",
            "         ...,\n",
            "         [146, 170, 246],\n",
            "         [157, 161, 249],\n",
            "         [171, 166, 255]]], dtype=uint8)\n",
            " array([[[  2,   9,  12],\n",
            "         [  0,   9,  12],\n",
            "         [  0,  10,  10],\n",
            "         ...,\n",
            "         [ 37,  49, 127],\n",
            "         [ 34,  47, 125],\n",
            "         [ 39,  55, 131]],\n",
            "\n",
            "        [[  0,   4,   7],\n",
            "         [  0,   6,   9],\n",
            "         [  0,   9,   9],\n",
            "         ...,\n",
            "         [ 40,  47, 126],\n",
            "         [ 39,  47, 124],\n",
            "         [ 44,  53, 127]],\n",
            "\n",
            "        [[  0,   3,   7],\n",
            "         [  2,   7,  10],\n",
            "         [  2,   9,  12],\n",
            "         ...,\n",
            "         [ 52,  52, 130],\n",
            "         [ 60,  52, 129],\n",
            "         [ 56,  46, 122]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[  2,   5,  13],\n",
            "         [  3,   5,  15],\n",
            "         [  1,   1,  17],\n",
            "         ...,\n",
            "         [255, 250, 255],\n",
            "         [255, 250, 255],\n",
            "         [255, 250, 255]],\n",
            "\n",
            "        [[  2,   8,   7],\n",
            "         [  1,   6,   7],\n",
            "         [  0,   3,   8],\n",
            "         ...,\n",
            "         [255, 250, 255],\n",
            "         [255, 250, 255],\n",
            "         [255, 250, 255]],\n",
            "\n",
            "        [[  0,   7,   2],\n",
            "         [  0,   4,   1],\n",
            "         [  0,   0,   1],\n",
            "         ...,\n",
            "         [255, 250, 255],\n",
            "         [255, 250, 255],\n",
            "         [255, 250, 255]]], dtype=uint8)\n",
            " array([[[ 54,  56,  90],\n",
            "         [ 57,  61,  85],\n",
            "         [ 74,  81,  84],\n",
            "         ...,\n",
            "         [ 97, 116, 143],\n",
            "         [ 80,  94, 122],\n",
            "         [ 98, 102, 131]],\n",
            "\n",
            "        [[ 84,  94, 118],\n",
            "         [ 60,  70,  87],\n",
            "         [ 60,  72,  74],\n",
            "         ...,\n",
            "         [ 88, 102, 125],\n",
            "         [ 86,  95, 122],\n",
            "         [ 93,  96, 124]],\n",
            "\n",
            "        [[ 52,  78,  85],\n",
            "         [ 54,  79,  83],\n",
            "         [ 65,  86,  84],\n",
            "         ...,\n",
            "         [ 78,  78,  96],\n",
            "         [ 84,  82, 104],\n",
            "         [ 82,  86, 110]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 27,  55,  72],\n",
            "         [ 46,  70,  92],\n",
            "         [ 50,  63,  95],\n",
            "         ...,\n",
            "         [ 21,  23, 135],\n",
            "         [ 26,  29, 138],\n",
            "         [ 30,  30, 138]],\n",
            "\n",
            "        [[ 41,  79,  97],\n",
            "         [ 40,  72,  95],\n",
            "         [ 52,  75, 107],\n",
            "         ...,\n",
            "         [ 29,  25, 138],\n",
            "         [ 25,  24, 134],\n",
            "         [ 24,  19, 128]],\n",
            "\n",
            "        [[ 19,  67,  85],\n",
            "         [ 31,  72,  97],\n",
            "         [ 49,  78, 115],\n",
            "         ...,\n",
            "         [ 20,  27, 130],\n",
            "         [ 18,  28, 129],\n",
            "         [ 16,  27, 125]]], dtype=uint8)\n",
            " array([[[  0,   0,  13],\n",
            "         [  0,   0,  14],\n",
            "         [  1,   2,  16],\n",
            "         ...,\n",
            "         [ 54,  25,  40],\n",
            "         [ 61,  20,  51],\n",
            "         [ 62,  18,  55]],\n",
            "\n",
            "        [[  0,   0,  11],\n",
            "         [  0,   1,  12],\n",
            "         [  1,   3,  14],\n",
            "         ...,\n",
            "         [ 55,  30,  40],\n",
            "         [ 55,  19,  43],\n",
            "         [ 56,  16,  44]],\n",
            "\n",
            "        [[  0,   0,  10],\n",
            "         [  0,   0,  10],\n",
            "         [  0,   2,  10],\n",
            "         ...,\n",
            "         [ 57,  36,  38],\n",
            "         [ 50,  25,  35],\n",
            "         [ 57,  29,  42]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 25,  29,  40],\n",
            "         [ 24,  29,  38],\n",
            "         [ 21,  32,  36],\n",
            "         ...,\n",
            "         [189, 151, 147],\n",
            "         [156, 151, 136],\n",
            "         [142, 155, 133]],\n",
            "\n",
            "        [[ 42,  31,  34],\n",
            "         [ 34,  27,  30],\n",
            "         [ 26,  25,  27],\n",
            "         ...,\n",
            "         [190, 149, 147],\n",
            "         [174, 151, 143],\n",
            "         [173, 159, 147]],\n",
            "\n",
            "        [[ 46,  28,  27],\n",
            "         [ 43,  29,  30],\n",
            "         [ 37,  31,  32],\n",
            "         ...,\n",
            "         [192, 148, 149],\n",
            "         [192, 160, 155],\n",
            "         [186, 158, 151]]], dtype=uint8)]\n",
            "48\n",
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(X_val))\n",
        "print(X_val[:5])\n",
        "print(len(y_val))\n",
        "print(y_val[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ql6Mh92QXNl9"
      },
      "outputs": [],
      "source": [
        "# Function to create patches from an image with a specified patch size\n",
        "def split_image(image, patch_size=256, qtd=1):\n",
        "\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Calculate the number of rows and columns needed for patches\n",
        "    num_rows = math.ceil(height / patch_size)\n",
        "    num_cols = math.ceil(width / patch_size)\n",
        "\n",
        "    out_height = patch_size * num_rows\n",
        "    out_width = patch_size * num_cols\n",
        "\n",
        "    # Calculate the amount of padding needed\n",
        "    pad_height = out_height - height\n",
        "    pad_width = out_width - width\n",
        "\n",
        "    # Pad the image with zeros if necessary\n",
        "    if pad_height > 0 or pad_width > 0:\n",
        "        image = cv.copyMakeBorder(image, 0, pad_height, 0, pad_width, cv.BORDER_CONSTANT, value=0)\n",
        "\n",
        "    # Initialize an empty list to store the patches\n",
        "    patches = []\n",
        "\n",
        "    # Iterate through the image and extract patches\n",
        "    for y in range(0, out_height, patch_size):\n",
        "        for x in range(0, out_width, patch_size):\n",
        "            patch = image[y:y+patch_size, x:x+patch_size]\n",
        "            patches.append(patch)\n",
        "\n",
        "    if qtd < 1 and qtd > 0:\n",
        "        np.random.shuffle(patches)\n",
        "        patches = patches[:int(len(patches)*qtd)]\n",
        "\n",
        "    return np.array(patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "siGNQzC1nilO"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "\n",
        "# # Define your NumPy array of images\n",
        "# image_array = ex_patches\n",
        "\n",
        "# # Loop through each image in the array\n",
        "# for i in range(len(image_array)):\n",
        "#     # Convert the NumPy array to a PIL image\n",
        "#     image = Image.fromarray(image_array[i])\n",
        "\n",
        "#     # Display the image in the Jupyter notebook\n",
        "#     display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0_8rSAr6jY2y"
      },
      "outputs": [],
      "source": [
        "def patchify(X, y, qtd_patches=1):\n",
        "    X_patches, y_patches = [], []\n",
        "    for i in range(len(X)):\n",
        "        patches = split_image(X[i], 256, qtd_patches)\n",
        "        X_patches.extend(patches)\n",
        "        for patch in patches:\n",
        "            if y[i] == 1:\n",
        "                y_patches.append(1)\n",
        "            else:\n",
        "                y_patches.append(0)\n",
        "    return np.array(X_patches), np.array(y_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "An1BF-ttXXC9"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = patchify(X_train, y_train, 0.05)\n",
        "X_test, y_test = patchify(X_test, y_test, 0.05)\n",
        "X_val, y_val = patchify(X_val, y_val, 0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists('datasets'):\n",
        "    os.makedirs('datasets')\n",
        "\n",
        "np.save('datasets/X_train.npy', X_train)\n",
        "np.save('datasets/y_train.npy', y_train)\n",
        "\n",
        "np.save('datasets/X_test.npy', X_test)\n",
        "np.save('datasets/y_test.npy', y_test)\n",
        "\n",
        "np.save('datasets/X_val.npy', X_val)\n",
        "np.save('datasets/y_val.npy', y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDc9CBqmdI5w",
        "outputId": "e69c8975-546d-48c5-e6d2-8381051b3609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1308\n",
            "451\n",
            "444\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(X_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_patches = np.load('datasets/X_train.npy')\n",
        "y_train_patches = np.load('datasets/y_train.npy')\n",
        "\n",
        "X_val_patches = np.load('datasets/X_val.npy')\n",
        "y_val_patches = np.load('datasets/y_val.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oAxB0xSpXb5l"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "if not os.path.exists('weights'):\n",
        "    os.mkdir('weights')\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='weights/checkpoint_path.h5',\n",
        "    monitor='val_loss',  # Métrica para monitorar (pode ser 'val_loss' para perda de validação)\n",
        "    save_best_only=True,  # Salva apenas o melhor modelo\n",
        "    mode='min',  # 'max' se a métrica monitorada deve ser maximizada, 'min' se deve ser minimizada\n",
        "    verbose=1  # Exibe mensagens durante o salvamento do modelo\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZRY39S1XdPW",
        "outputId": "56873b01-6295-4f03-9163-7fa518479f91"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import certifi\n",
        "\n",
        "model = tf.keras.applications.densenet.DenseNet121(\n",
        "    include_top=False,\n",
        "    weights=None,\n",
        "    input_shape=(256, 256, 3)\n",
        ")\n",
        "model.load_weights(os.path.join(pasta_do_notebook, 'weights', 'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'))\n",
        "\n",
        "\n",
        "top = GlobalAveragePooling2D()(model.output)\n",
        "top = Dense(1024, activation='relu')(top)\n",
        "top = Dense(1, activation='sigmoid')(top)\n",
        "model = Model(inputs=model.input, outputs=top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x6crICgisol7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.densenet import preprocess_input\n",
        "def preprocess_data(X, y, shuffle=False):\n",
        "    # Shuffle the data if specified\n",
        "    X = X/255\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(X))\n",
        "        np.random.shuffle(indices)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whz0GVOWBrgl",
        "outputId": "1c2bd75e-194e-49fd-dd4b-676d9cc9bc41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1308, 256, 256, 3) (451, 256, 256, 3) (444, 256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = preprocess_data(X_train, y_train)\n",
        "X_test, y_test = preprocess_data(X_test, y_test)\n",
        "X_val, y_val = preprocess_data(X_val, y_val)\n",
        "print(X_train.shape, X_test.shape, X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sy1MiuhABFqb"
      },
      "outputs": [],
      "source": [
        "# Convert NumPy arrays to TensorFlow tensors\n",
        "X_train = tf.convert_to_tensor(X_train)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "\n",
        "# Create a tf.data.Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "\n",
        "# Batch, shuffle, and repeat the dataset\n",
        "batch_size = 16\n",
        "dataset = dataset.shuffle(buffer_size=len(X_train))\n",
        "dataset = dataset.batch(batch_size)\n",
        "# dataset = dataset.repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfjwFSPGXevH",
        "outputId": "c98688f1-ee9d-431c-9943-f3e9d47acba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.8479\n",
            "Epoch 00001: val_loss improved from inf to 15.78632, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 53s 491ms/step - loss: 0.4194 - accuracy: 0.8479 - val_loss: 15.7863 - val_accuracy: 0.5000\n",
            "Epoch 2/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9090\n",
            "Epoch 00002: val_loss improved from 15.78632 to 4.78399, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 34s 414ms/step - loss: 0.2449 - accuracy: 0.9090 - val_loss: 4.7840 - val_accuracy: 0.5023\n",
            "Epoch 3/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.9067\n",
            "Epoch 00003: val_loss improved from 4.78399 to 3.75831, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 33s 405ms/step - loss: 0.2237 - accuracy: 0.9067 - val_loss: 3.7583 - val_accuracy: 0.5541\n",
            "Epoch 4/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9174\n",
            "Epoch 00004: val_loss did not improve from 3.75831\n",
            "82/82 [==============================] - 15s 174ms/step - loss: 0.2138 - accuracy: 0.9174 - val_loss: 9.0160 - val_accuracy: 0.7928\n",
            "Epoch 5/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1971 - accuracy: 0.9235\n",
            "Epoch 00005: val_loss improved from 3.75831 to 0.85607, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 33s 404ms/step - loss: 0.1971 - accuracy: 0.9235 - val_loss: 0.8561 - val_accuracy: 0.8356\n",
            "Epoch 6/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9274\n",
            "Epoch 00006: val_loss did not improve from 0.85607\n",
            "82/82 [==============================] - 15s 176ms/step - loss: 0.1953 - accuracy: 0.9274 - val_loss: 1.4678 - val_accuracy: 0.8446\n",
            "Epoch 7/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.2042 - accuracy: 0.9190\n",
            "Epoch 00007: val_loss improved from 0.85607 to 0.20554, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 34s 418ms/step - loss: 0.2042 - accuracy: 0.9190 - val_loss: 0.2055 - val_accuracy: 0.9189\n",
            "Epoch 8/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9335\n",
            "Epoch 00008: val_loss improved from 0.20554 to 0.10334, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 34s 407ms/step - loss: 0.1703 - accuracy: 0.9335 - val_loss: 0.1033 - val_accuracy: 0.9685\n",
            "Epoch 9/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9335\n",
            "Epoch 00009: val_loss did not improve from 0.10334\n",
            "82/82 [==============================] - 15s 174ms/step - loss: 0.1677 - accuracy: 0.9335 - val_loss: 0.3492 - val_accuracy: 0.8536\n",
            "Epoch 10/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9327\n",
            "Epoch 00010: val_loss did not improve from 0.10334\n",
            "82/82 [==============================] - 14s 173ms/step - loss: 0.1534 - accuracy: 0.9327 - val_loss: 0.1911 - val_accuracy: 0.9302\n",
            "Epoch 11/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.2162 - accuracy: 0.9251\n",
            "Epoch 00011: val_loss did not improve from 0.10334\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.2162 - accuracy: 0.9251 - val_loss: 0.3714 - val_accuracy: 0.8243\n",
            "Epoch 12/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9511\n",
            "Epoch 00012: val_loss improved from 0.10334 to 0.08389, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 34s 412ms/step - loss: 0.1265 - accuracy: 0.9511 - val_loss: 0.0839 - val_accuracy: 0.9752\n",
            "Epoch 13/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9518\n",
            "Epoch 00013: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 15s 174ms/step - loss: 0.1347 - accuracy: 0.9518 - val_loss: 0.2446 - val_accuracy: 0.8896\n",
            "Epoch 14/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1084 - accuracy: 0.9587\n",
            "Epoch 00014: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 173ms/step - loss: 0.1084 - accuracy: 0.9587 - val_loss: 0.2123 - val_accuracy: 0.9279\n",
            "Epoch 15/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9534\n",
            "Epoch 00015: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.1366 - accuracy: 0.9534 - val_loss: 0.6123 - val_accuracy: 0.8018\n",
            "Epoch 16/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9633\n",
            "Epoch 00016: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.1130 - accuracy: 0.9633 - val_loss: 0.2789 - val_accuracy: 0.8964\n",
            "Epoch 17/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9610\n",
            "Epoch 00017: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.1190 - accuracy: 0.9610 - val_loss: 0.2415 - val_accuracy: 0.9009\n",
            "Epoch 18/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9633\n",
            "Epoch 00018: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.1057 - accuracy: 0.9633 - val_loss: 0.2399 - val_accuracy: 0.9009\n",
            "Epoch 19/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9549\n",
            "Epoch 00019: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.1453 - accuracy: 0.9549 - val_loss: 0.1470 - val_accuracy: 0.9302\n",
            "Epoch 20/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9664\n",
            "Epoch 00020: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0973 - accuracy: 0.9664 - val_loss: 0.1567 - val_accuracy: 0.9505\n",
            "Epoch 21/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9656\n",
            "Epoch 00021: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0973 - accuracy: 0.9656 - val_loss: 0.1838 - val_accuracy: 0.9077\n",
            "Epoch 22/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9656\n",
            "Epoch 00022: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0905 - accuracy: 0.9656 - val_loss: 0.1005 - val_accuracy: 0.9595\n",
            "Epoch 23/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9671\n",
            "Epoch 00023: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.1076 - accuracy: 0.9671 - val_loss: 0.2694 - val_accuracy: 0.8784\n",
            "Epoch 24/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9748\n",
            "Epoch 00024: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0687 - accuracy: 0.9748 - val_loss: 0.1235 - val_accuracy: 0.9459\n",
            "Epoch 25/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9794\n",
            "Epoch 00025: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0564 - accuracy: 0.9794 - val_loss: 0.2587 - val_accuracy: 0.9144\n",
            "Epoch 26/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9709\n",
            "Epoch 00026: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0717 - accuracy: 0.9709 - val_loss: 0.2159 - val_accuracy: 0.9032\n",
            "Epoch 27/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9717\n",
            "Epoch 00027: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0856 - accuracy: 0.9717 - val_loss: 0.2659 - val_accuracy: 0.9032\n",
            "Epoch 28/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9717\n",
            "Epoch 00028: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0675 - accuracy: 0.9717 - val_loss: 0.1967 - val_accuracy: 0.9482\n",
            "Epoch 29/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9610\n",
            "Epoch 00029: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.1020 - accuracy: 0.9610 - val_loss: 0.2554 - val_accuracy: 0.9122\n",
            "Epoch 30/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9740\n",
            "Epoch 00030: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0779 - accuracy: 0.9740 - val_loss: 0.6206 - val_accuracy: 0.8176\n",
            "Epoch 31/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9687\n",
            "Epoch 00031: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0809 - accuracy: 0.9687 - val_loss: 0.5932 - val_accuracy: 0.8333\n",
            "Epoch 32/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9778\n",
            "Epoch 00032: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0571 - accuracy: 0.9778 - val_loss: 0.3499 - val_accuracy: 0.8671\n",
            "Epoch 33/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9755\n",
            "Epoch 00033: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0579 - accuracy: 0.9755 - val_loss: 0.8674 - val_accuracy: 0.8153\n",
            "Epoch 34/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.9763\n",
            "Epoch 00034: val_loss did not improve from 0.08389\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0592 - accuracy: 0.9763 - val_loss: 0.4189 - val_accuracy: 0.8784\n",
            "Epoch 35/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9687\n",
            "Epoch 00035: val_loss improved from 0.08389 to 0.05899, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 34s 414ms/step - loss: 0.0834 - accuracy: 0.9687 - val_loss: 0.0590 - val_accuracy: 0.9730\n",
            "Epoch 36/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9763\n",
            "Epoch 00036: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 15s 174ms/step - loss: 0.0539 - accuracy: 0.9763 - val_loss: 3.7893 - val_accuracy: 0.8671\n",
            "Epoch 37/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9702\n",
            "Epoch 00037: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 176ms/step - loss: 0.0733 - accuracy: 0.9702 - val_loss: 2.7042 - val_accuracy: 0.8153\n",
            "Epoch 38/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9633\n",
            "Epoch 00038: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0921 - accuracy: 0.9633 - val_loss: 1.8445 - val_accuracy: 0.5743\n",
            "Epoch 39/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9748\n",
            "Epoch 00039: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0663 - accuracy: 0.9748 - val_loss: 1.5111 - val_accuracy: 0.8243\n",
            "Epoch 40/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9786\n",
            "Epoch 00040: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0612 - accuracy: 0.9786 - val_loss: 0.7759 - val_accuracy: 0.7770\n",
            "Epoch 41/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9503\n",
            "Epoch 00041: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.1299 - accuracy: 0.9503 - val_loss: 0.2318 - val_accuracy: 0.9144\n",
            "Epoch 42/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9725\n",
            "Epoch 00042: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0811 - accuracy: 0.9725 - val_loss: 0.1261 - val_accuracy: 0.9617\n",
            "Epoch 43/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9740\n",
            "Epoch 00043: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0668 - accuracy: 0.9740 - val_loss: 0.5567 - val_accuracy: 0.8649\n",
            "Epoch 44/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9572\n",
            "Epoch 00044: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.1171 - accuracy: 0.9572 - val_loss: 0.3049 - val_accuracy: 0.8761\n",
            "Epoch 45/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9732\n",
            "Epoch 00045: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0812 - accuracy: 0.9732 - val_loss: 0.1845 - val_accuracy: 0.9505\n",
            "Epoch 46/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9755\n",
            "Epoch 00046: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0563 - accuracy: 0.9755 - val_loss: 0.0951 - val_accuracy: 0.9640\n",
            "Epoch 47/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9817\n",
            "Epoch 00047: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0438 - accuracy: 0.9817 - val_loss: 0.1495 - val_accuracy: 0.9414\n",
            "Epoch 48/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9786\n",
            "Epoch 00048: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0631 - accuracy: 0.9786 - val_loss: 0.1934 - val_accuracy: 0.9077\n",
            "Epoch 49/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9755\n",
            "Epoch 00049: val_loss did not improve from 0.05899\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0654 - accuracy: 0.9755 - val_loss: 0.1078 - val_accuracy: 0.9550\n",
            "Epoch 50/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9809\n",
            "Epoch 00050: val_loss improved from 0.05899 to 0.04468, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 33s 411ms/step - loss: 0.0484 - accuracy: 0.9809 - val_loss: 0.0447 - val_accuracy: 0.9842\n",
            "Epoch 51/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9832\n",
            "Epoch 00051: val_loss improved from 0.04468 to 0.03462, saving model to weightscheckpoint_path.h5\n",
            "82/82 [==============================] - 35s 423ms/step - loss: 0.0473 - accuracy: 0.9832 - val_loss: 0.0346 - val_accuracy: 0.9887\n",
            "Epoch 52/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9679\n",
            "Epoch 00052: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 15s 176ms/step - loss: 0.0776 - accuracy: 0.9679 - val_loss: 0.3184 - val_accuracy: 0.8896\n",
            "Epoch 53/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9839\n",
            "Epoch 00053: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0527 - accuracy: 0.9839 - val_loss: 0.1182 - val_accuracy: 0.9527\n",
            "Epoch 54/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9786\n",
            "Epoch 00054: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0562 - accuracy: 0.9786 - val_loss: 0.0482 - val_accuracy: 0.9842\n",
            "Epoch 55/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9878\n",
            "Epoch 00055: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0382 - accuracy: 0.9878 - val_loss: 0.0861 - val_accuracy: 0.9662\n",
            "Epoch 56/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9931\n",
            "Epoch 00056: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0244 - accuracy: 0.9931 - val_loss: 0.2104 - val_accuracy: 0.9279\n",
            "Epoch 57/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9786\n",
            "Epoch 00057: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0576 - accuracy: 0.9786 - val_loss: 0.0390 - val_accuracy: 0.9910\n",
            "Epoch 58/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9870\n",
            "Epoch 00058: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0403 - accuracy: 0.9870 - val_loss: 0.1310 - val_accuracy: 0.9482\n",
            "Epoch 59/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9771\n",
            "Epoch 00059: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0558 - accuracy: 0.9771 - val_loss: 0.0543 - val_accuracy: 0.9842\n",
            "Epoch 60/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9679\n",
            "Epoch 00060: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0835 - accuracy: 0.9679 - val_loss: 0.5178 - val_accuracy: 0.7883\n",
            "Epoch 61/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9801\n",
            "Epoch 00061: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0492 - accuracy: 0.9801 - val_loss: 0.1071 - val_accuracy: 0.9640\n",
            "Epoch 62/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9794\n",
            "Epoch 00062: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0572 - accuracy: 0.9794 - val_loss: 0.0924 - val_accuracy: 0.9730\n",
            "Epoch 63/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9832\n",
            "Epoch 00063: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0441 - accuracy: 0.9832 - val_loss: 0.0543 - val_accuracy: 0.9887\n",
            "Epoch 64/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9824\n",
            "Epoch 00064: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0470 - accuracy: 0.9824 - val_loss: 0.0618 - val_accuracy: 0.9730\n",
            "Epoch 65/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9855\n",
            "Epoch 00065: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0448 - accuracy: 0.9855 - val_loss: 0.2271 - val_accuracy: 0.9189\n",
            "Epoch 66/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9862\n",
            "Epoch 00066: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0396 - accuracy: 0.9862 - val_loss: 0.0985 - val_accuracy: 0.9662\n",
            "Epoch 67/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9878\n",
            "Epoch 00067: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0342 - accuracy: 0.9878 - val_loss: 0.0893 - val_accuracy: 0.9662\n",
            "Epoch 68/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9901\n",
            "Epoch 00068: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0234 - accuracy: 0.9901 - val_loss: 0.1137 - val_accuracy: 0.9685\n",
            "Epoch 69/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9602\n",
            "Epoch 00069: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.1013 - accuracy: 0.9602 - val_loss: 0.9494 - val_accuracy: 0.6757\n",
            "Epoch 70/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9641\n",
            "Epoch 00070: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0980 - accuracy: 0.9641 - val_loss: 0.1444 - val_accuracy: 0.9257\n",
            "Epoch 71/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9862\n",
            "Epoch 00071: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0363 - accuracy: 0.9862 - val_loss: 0.1031 - val_accuracy: 0.9640\n",
            "Epoch 72/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9839\n",
            "Epoch 00072: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0475 - accuracy: 0.9839 - val_loss: 0.1062 - val_accuracy: 0.9550\n",
            "Epoch 73/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9855\n",
            "Epoch 00073: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0492 - accuracy: 0.9855 - val_loss: 0.1820 - val_accuracy: 0.9257\n",
            "Epoch 74/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9809\n",
            "Epoch 00074: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0506 - accuracy: 0.9809 - val_loss: 0.1914 - val_accuracy: 0.9234\n",
            "Epoch 75/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9862\n",
            "Epoch 00075: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0357 - accuracy: 0.9862 - val_loss: 0.0759 - val_accuracy: 0.9730\n",
            "Epoch 76/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9885\n",
            "Epoch 00076: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0259 - accuracy: 0.9885 - val_loss: 0.0667 - val_accuracy: 0.9730\n",
            "Epoch 77/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9855\n",
            "Epoch 00077: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0418 - accuracy: 0.9855 - val_loss: 0.1255 - val_accuracy: 0.9550\n",
            "Epoch 78/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9893\n",
            "Epoch 00078: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0367 - accuracy: 0.9893 - val_loss: 0.0815 - val_accuracy: 0.9640\n",
            "Epoch 79/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9908\n",
            "Epoch 00079: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0237 - accuracy: 0.9908 - val_loss: 0.0720 - val_accuracy: 0.9685\n",
            "Epoch 80/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9748\n",
            "Epoch 00080: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 175ms/step - loss: 0.0625 - accuracy: 0.9748 - val_loss: 0.1394 - val_accuracy: 0.9550\n",
            "Epoch 81/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9916\n",
            "Epoch 00081: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0221 - accuracy: 0.9916 - val_loss: 0.0463 - val_accuracy: 0.9865\n",
            "Epoch 82/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9916\n",
            "Epoch 00082: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0246 - accuracy: 0.9916 - val_loss: 0.0798 - val_accuracy: 0.9730\n",
            "Epoch 83/100\n",
            "82/82 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9870\n",
            "Epoch 00083: val_loss did not improve from 0.03462\n",
            "82/82 [==============================] - 14s 174ms/step - loss: 0.0351 - accuracy: 0.9870 - val_loss: 0.0995 - val_accuracy: 0.9707\n"
          ]
        }
      ],
      "source": [
        "# Compile o modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treine o modelo\n",
        "# r = model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=50, callbacks=[checkpoint], batch_size = 8)\n",
        "r = model.fit(dataset, validation_data=(X_val,y_val), epochs=100, callbacks=[checkpoint])\n",
        "\n",
        "# Avalie o modelo no conjunto de teste\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Exiba as métricas calculadas\n",
        "print(\"Loss:\", test_loss)\n",
        "print(\"Accuracy:\", test_accuracy)\n",
        "print(\"Validation\", r.history['val_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5OxTop9Xgp2"
      },
      "outputs": [],
      "source": [
        "loss = r.history['loss']\n",
        "val_loss = r.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxjnm_5vXiIV"
      },
      "outputs": [],
      "source": [
        "acc1 = r.history['accuracy']\n",
        "val_acc1 = r.history['val_accuracy']\n",
        "plt.plot(epochs, acc1, 'y', label='Training acurácia')\n",
        "plt.plot(epochs, val_acc1, 'r', label='Validation acurácia')\n",
        "plt.title('Training and validation acurácia')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('acurácia')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = np.load('datasets/X_test.npy')\n",
        "y_test = np.load('datasets/y_test.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "451\n"
          ]
        }
      ],
      "source": [
        "print(len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "model = tf.keras.applications.densenet.DenseNet121(\n",
        "    include_top=False,\n",
        "    weights=None,\n",
        "    input_shape=(256, 256, 3)\n",
        ")\n",
        "\n",
        "top = GlobalAveragePooling2D()(model.output)\n",
        "top = Dense(1024, activation='relu')(top)\n",
        "top = Dense(1, activation='sigmoid')(top)\n",
        "model = Model(inputs=model.input, outputs=top)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.applications.densenet import preprocess_input\n",
        "def preprocess_data(X, y, shuffle=False):\n",
        "    # Shuffle the data if specified\n",
        "    X = X/255\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(X))\n",
        "        np.random.shuffle(indices)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test, y_test = preprocess_data(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9401\n",
            "Precision: 0.9345\n",
            "Recall: 0.9469\n",
            "F1 Score: 0.9407\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model.load_weights(os.path.join(pasta_do_notebook, 'weights', 'noise_detect_checkpoint_path.h5'))\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred_classes = np.round(y_pred)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes)\n",
        "recall = recall_score(y_test, y_pred_classes)\n",
        "f1 = f1_score(y_test, y_pred_classes)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
