{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHXIgcrGVzG8",
        "outputId": "b9e90105-5504-4a27-c9b1-5e92def7b7b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.pyplot._IonContext at 0x1c7427a5348>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "from PIL import Image\n",
        "import re\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "plt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jawA44PlcK4b"
      },
      "source": [
        "BOTAR AQUI SUA PASTA DO NOTEBOOK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u3FVa380cKR0"
      },
      "outputs": [],
      "source": [
        "# pasta_do_notebook = '/content/drive/MyDrive/Blur_CLas'\n",
        "pasta_do_notebook = '.'\n",
        "os.chdir(pasta_do_notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "O37XW_9Mbt9e",
        "outputId": "1f528eae-77d4-4c09-cb60-f7a6b05eeb3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\daniel_moreira\\\\reconhecimento_de_padroes\\\\reconhecimento_de_padroes\\\\blurdetect'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TwZzcjiiWpQe"
      },
      "outputs": [],
      "source": [
        "image_path = []\n",
        "\n",
        "base_path = 'blur_dataset'\n",
        "for caminho in os.listdir(base_path):\n",
        "  pasta = os.path.join(base_path, caminho)\n",
        "  for image in os.listdir(pasta):\n",
        "    image_path = os.path.join(pasta, image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cjTzYf6UWrBW"
      },
      "outputs": [],
      "source": [
        "defocused_path = 'blur_dataset/defocused_blurred/*'\n",
        "motion_path = 'blur_dataset/motion_blurred/*'\n",
        "sharp_path = 'blur_dataset/sharp/*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gDah8dg2WsmN"
      },
      "outputs": [],
      "source": [
        "ids = []\n",
        "\n",
        "for caminho in glob.glob(sharp_path):\n",
        "  partes = caminho.split('_')\n",
        "  partes = partes[:-1]\n",
        "  res = \"_\".join(partes)\n",
        "  res = os.path.basename(res)\n",
        "  ids.append(res)\n",
        "\n",
        "ids.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAiC1S5LfaId",
        "outputId": "e2562ae2-2b6f-4b5f-84aa-526bec01b12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0_IPHONE-SE', '100_NIKON-D3400-35MM', '101_NIKON-D3400-35MM', '102_NIKON-D3400-35MM', '103_HUAWEI-P20']\n",
            "['96_HONOR-7X', '97_HONOR-7X', '98_HONOR-7X', '99_HONOR-7X', '9_HUAWEI-P20']\n"
          ]
        }
      ],
      "source": [
        "print(ids[:5])\n",
        "print(ids[-5:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jdwaqiy0WvV2"
      },
      "outputs": [],
      "source": [
        "split = int(0.8 * len(ids))\n",
        "id_train, id_test = ids[:split], ids[split:]\n",
        "split = int(0.5 * len(id_test))\n",
        "id_test, id_val = id_test[:split], id_test[split:]\n",
        "\n",
        "X_train, y_train = [], []\n",
        "X_test, y_test = [], []\n",
        "X_val, y_val = [], []\n",
        "\n",
        "for caminho in glob.glob(defocused_path):\n",
        "    if os.path.basename(caminho).split('.')[0][:-2] in id_train:\n",
        "        X_train.append(caminho)\n",
        "        y_train.append(1)\n",
        "    elif os.path.basename(caminho).split('.')[0][:-2] in id_test:\n",
        "        X_test.append(caminho)\n",
        "        y_test.append(1)\n",
        "    elif os.path.basename(caminho).split('.')[0][:-2] in id_val:\n",
        "        X_val.append(caminho)\n",
        "        y_val.append(1)\n",
        "\n",
        "for caminho in glob.glob(motion_path):\n",
        "    if os.path.basename(caminho).split('.')[0][:-2] in id_train:\n",
        "        X_train.append(caminho)\n",
        "        y_train.append(1)\n",
        "    elif os.path.basename(caminho).split('.')[0][:-2] in id_test:\n",
        "        X_test.append(caminho)\n",
        "        y_test.append(1)\n",
        "    elif os.path.basename(caminho).split('.')[0][:-2] in id_val:\n",
        "        X_val.append(caminho)\n",
        "        y_val.append(1)\n",
        "\n",
        "for caminho in glob.glob(sharp_path):\n",
        "    if os.path.basename(caminho).split('.')[0][:-2] in id_train:\n",
        "        X_train.append(caminho)\n",
        "        y_train.append(0)\n",
        "    elif os.path.basename(caminho).split('.')[0][:-2] in id_test:\n",
        "        X_test.append(caminho)\n",
        "        y_test.append(0)\n",
        "    elif os.path.basename(caminho).split('.')[0][:-2] in id_val:\n",
        "        X_val.append(caminho)\n",
        "        y_val.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYaCKO2PiHsf",
        "outputId": "50190797-54fb-4de2-cb6a-dbac3e7b5952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "831\n",
            "105\n",
            "105\n",
            "['blur_dataset/defocused_blurred\\\\69_SAMSUNG-GALAXY-J3_F.jpg', 'blur_dataset/defocused_blurred\\\\6_HUAWEI-MATE20_F.jpg', 'blur_dataset/defocused_blurred\\\\70_HONOR-8X_F.jpg', 'blur_dataset/defocused_blurred\\\\71_SAMSUNG-GALAXY-A6_F.jpg', 'blur_dataset/defocused_blurred\\\\72_IPHONE-8_F.jpg']\n",
            "105\n",
            "[1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(X_val))\n",
        "print(X_val[:5])\n",
        "print(len(y_val))\n",
        "print(y_val[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ql6Mh92QXNl9"
      },
      "outputs": [],
      "source": [
        "# Function to create patches from an image with a specified patch size\n",
        "def split_image(image, patch_size=256, qtd=1):\n",
        "\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Calculate the number of rows and columns needed for patches\n",
        "    num_rows = math.ceil(height / patch_size)\n",
        "    num_cols = math.ceil(width / patch_size)\n",
        "\n",
        "    out_height = patch_size * num_rows\n",
        "    out_width = patch_size * num_cols\n",
        "\n",
        "    # Calculate the amount of padding needed\n",
        "    pad_height = out_height - height\n",
        "    pad_width = out_width - width\n",
        "\n",
        "    # Pad the image with zeros if necessary\n",
        "    if pad_height > 0 or pad_width > 0:\n",
        "        image = cv.copyMakeBorder(image, 0, pad_height, 0, pad_width, cv.BORDER_CONSTANT, value=0)\n",
        "\n",
        "    # Initialize an empty list to store the patches\n",
        "    patches = []\n",
        "\n",
        "    # Iterate through the image and extract patches\n",
        "    for y in range(0, out_height, patch_size):\n",
        "        for x in range(0, out_width, patch_size):\n",
        "            patch = image[y:y+patch_size, x:x+patch_size]\n",
        "            patches.append(patch)\n",
        "\n",
        "    if qtd < 1 and qtd > 0:\n",
        "        np.random.shuffle(patches)\n",
        "        patches = patches[:int(len(patches)*qtd)]\n",
        "\n",
        "    return np.array(patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkYMeykElL5t",
        "outputId": "a5098664-9be1-4682-b957-dcf8207d5143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5120, 3840, 3)\n",
            "(60, 256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "ex_image = cv.imread(X_train[-1])\n",
        "ex_patches = split_image(ex_image, 256, 0.2)\n",
        "print(ex_image.shape)\n",
        "print(ex_patches.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "siGNQzC1nilO"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "\n",
        "# # Define your NumPy array of images\n",
        "# image_array = ex_patches\n",
        "\n",
        "# # Loop through each image in the array\n",
        "# for i in range(len(image_array)):\n",
        "#     # Convert the NumPy array to a PIL image\n",
        "#     image = Image.fromarray(image_array[i])\n",
        "\n",
        "#     # Display the image in the Jupyter notebook\n",
        "#     display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0_8rSAr6jY2y"
      },
      "outputs": [],
      "source": [
        "def patchify(X, y, qtd_patches=1):\n",
        "    X_patches, y_patches = [], []\n",
        "    for i in range(len(X)):\n",
        "        patches = split_image(cv.imread(X[i]), 256, qtd_patches)\n",
        "        X_patches.extend(patches)\n",
        "        for patch in patches:\n",
        "            if y[i] == 1:\n",
        "                y_patches.append(1)\n",
        "            else:\n",
        "                y_patches.append(0)\n",
        "    return np.array(X_patches), np.array(y_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An1BF-ttXXC9"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = patchify(X_train, y_train, 0.01)\n",
        "X_test, y_test = patchify(X_test, y_test, 0.01)\n",
        "X_val, y_val = patchify(X_val, y_val, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists('datasets'):\n",
        "    os.makedirs('datasets')\n",
        "\n",
        "np.save('datasets/X_train.npy', X_train)\n",
        "np.save('datasets/y_train.npy', y_train)\n",
        "\n",
        "np.save('datasets/X_test.npy', X_test)\n",
        "np.save('datasets/y_test.npy', y_test)\n",
        "\n",
        "np.save('datasets/X_val.npy', X_val)\n",
        "np.save('datasets/y_val.npy', y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDc9CBqmdI5w",
        "outputId": "e69c8975-546d-48c5-e6d2-8381051b3609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1482\n",
            "213\n",
            "204\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(X_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_patches = np.load('datasets/X_train.npy')\n",
        "y_train_patches = np.load('datasets/y_train.npy')\n",
        "\n",
        "X_val_patches = np.load('datasets/X_val.npy')\n",
        "y_val_patches = np.load('datasets/y_val.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAxB0xSpXb5l"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "if not os.path.exists('weights'):\n",
        "    os.mkdir('weights')\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='weightscheckpoint_path.h5',\n",
        "    monitor='val_loss',  # Métrica para monitorar (pode ser 'val_loss' para perda de validação)\n",
        "    save_best_only=True,  # Salva apenas o melhor modelo\n",
        "    mode='min',  # 'max' se a métrica monitorada deve ser maximizada, 'min' se deve ser minimizada\n",
        "    verbose=1  # Exibe mensagens durante o salvamento do modelo\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZRY39S1XdPW",
        "outputId": "56873b01-6295-4f03-9163-7fa518479f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29084464/29084464 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "model = tf.keras.applications.densenet.DenseNet121(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(256, 256, 3)\n",
        ")\n",
        "\n",
        "top = GlobalAveragePooling2D()(model.output)\n",
        "top = Dense(1024, activation='relu')(top)\n",
        "top = Dense(1, activation='sigmoid')(top)\n",
        "model = Model(inputs=model.input, outputs=top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6crICgisol7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.densenet import preprocess_input\n",
        "def preprocess_data(X, y, shuffle=False):\n",
        "    # Shuffle the data if specified\n",
        "    X = X/255\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(X))\n",
        "        np.random.shuffle(indices)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whz0GVOWBrgl",
        "outputId": "1c2bd75e-194e-49fd-dd4b-676d9cc9bc41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1482, 256, 256, 3) (213, 256, 256, 3) (204, 256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = preprocess_data(X_train, y_train)\n",
        "X_test, y_test = preprocess_data(X_test, y_test)\n",
        "X_val, y_val = preprocess_data(X_val, y_val)\n",
        "print(X_train.shape, X_test.shape, X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy1MiuhABFqb"
      },
      "outputs": [],
      "source": [
        "# Convert NumPy arrays to TensorFlow tensors\n",
        "X_train = tf.convert_to_tensor(X_train)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "\n",
        "# Create a tf.data.Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "\n",
        "# Batch, shuffle, and repeat the dataset\n",
        "batch_size = 16\n",
        "dataset = dataset.shuffle(buffer_size=len(X_train))\n",
        "dataset = dataset.batch(batch_size)\n",
        "# dataset = dataset.repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfjwFSPGXevH",
        "outputId": "c98688f1-ee9d-431c-9943-f3e9d47acba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.6717 - accuracy: 0.7274\n",
            "Epoch 1: val_loss improved from inf to 0.68267, saving model to checkpoint_path.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r93/93 [==============================] - 109s 386ms/step - loss: 0.6717 - accuracy: 0.7274 - val_loss: 0.6827 - val_accuracy: 0.7059\n",
            "Epoch 2/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.7713\n",
            "Epoch 2: val_loss did not improve from 0.68267\n",
            "93/93 [==============================] - 23s 243ms/step - loss: 0.5322 - accuracy: 0.7713 - val_loss: 0.6997 - val_accuracy: 0.7255\n",
            "Epoch 3/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.5241 - accuracy: 0.7632\n",
            "Epoch 3: val_loss did not improve from 0.68267\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.5241 - accuracy: 0.7632 - val_loss: 2.0463 - val_accuracy: 0.6176\n",
            "Epoch 4/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.7989\n",
            "Epoch 4: val_loss improved from 0.68267 to 0.54985, saving model to checkpoint_path.h5\n",
            "93/93 [==============================] - 26s 284ms/step - loss: 0.4708 - accuracy: 0.7989 - val_loss: 0.5499 - val_accuracy: 0.7696\n",
            "Epoch 5/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.7982\n",
            "Epoch 5: val_loss improved from 0.54985 to 0.49693, saving model to checkpoint_path.h5\n",
            "93/93 [==============================] - 27s 290ms/step - loss: 0.4612 - accuracy: 0.7982 - val_loss: 0.4969 - val_accuracy: 0.7745\n",
            "Epoch 6/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4500 - accuracy: 0.8077\n",
            "Epoch 6: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 23s 249ms/step - loss: 0.4500 - accuracy: 0.8077 - val_loss: 0.5826 - val_accuracy: 0.7892\n",
            "Epoch 7/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4503 - accuracy: 0.8090\n",
            "Epoch 7: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 22s 241ms/step - loss: 0.4503 - accuracy: 0.8090 - val_loss: 0.5285 - val_accuracy: 0.7647\n",
            "Epoch 8/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4328 - accuracy: 0.8178\n",
            "Epoch 8: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 22s 240ms/step - loss: 0.4328 - accuracy: 0.8178 - val_loss: 0.5488 - val_accuracy: 0.7794\n",
            "Epoch 9/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.8347\n",
            "Epoch 9: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 23s 246ms/step - loss: 0.4210 - accuracy: 0.8347 - val_loss: 0.5746 - val_accuracy: 0.7892\n",
            "Epoch 10/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.8286\n",
            "Epoch 10: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 22s 238ms/step - loss: 0.4310 - accuracy: 0.8286 - val_loss: 0.6248 - val_accuracy: 0.7745\n",
            "Epoch 11/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4225 - accuracy: 0.8178\n",
            "Epoch 11: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 23s 243ms/step - loss: 0.4225 - accuracy: 0.8178 - val_loss: 240.3231 - val_accuracy: 0.6716\n",
            "Epoch 12/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.8212\n",
            "Epoch 12: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 22s 240ms/step - loss: 0.4453 - accuracy: 0.8212 - val_loss: 2.9244 - val_accuracy: 0.6863\n",
            "Epoch 13/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.8192\n",
            "Epoch 13: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 24s 254ms/step - loss: 0.4340 - accuracy: 0.8192 - val_loss: 0.7532 - val_accuracy: 0.6667\n",
            "Epoch 14/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4200 - accuracy: 0.8273\n",
            "Epoch 14: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 22s 238ms/step - loss: 0.4200 - accuracy: 0.8273 - val_loss: 0.5980 - val_accuracy: 0.7451\n",
            "Epoch 15/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8192\n",
            "Epoch 15: val_loss did not improve from 0.49693\n",
            "93/93 [==============================] - 22s 240ms/step - loss: 0.4268 - accuracy: 0.8192 - val_loss: 0.5729 - val_accuracy: 0.7794\n",
            "Epoch 16/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4140 - accuracy: 0.8313\n",
            "Epoch 16: val_loss improved from 0.49693 to 0.49536, saving model to checkpoint_path.h5\n",
            "93/93 [==============================] - 26s 281ms/step - loss: 0.4140 - accuracy: 0.8313 - val_loss: 0.4954 - val_accuracy: 0.8039\n",
            "Epoch 17/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3820 - accuracy: 0.8448\n",
            "Epoch 17: val_loss did not improve from 0.49536\n",
            "93/93 [==============================] - 23s 243ms/step - loss: 0.3820 - accuracy: 0.8448 - val_loss: 0.5232 - val_accuracy: 0.7990\n",
            "Epoch 18/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.8354\n",
            "Epoch 18: val_loss improved from 0.49536 to 0.48034, saving model to checkpoint_path.h5\n",
            "93/93 [==============================] - 29s 310ms/step - loss: 0.3953 - accuracy: 0.8354 - val_loss: 0.4803 - val_accuracy: 0.8137\n",
            "Epoch 19/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3887 - accuracy: 0.8428\n",
            "Epoch 19: val_loss did not improve from 0.48034\n",
            "93/93 [==============================] - 23s 242ms/step - loss: 0.3887 - accuracy: 0.8428 - val_loss: 0.5132 - val_accuracy: 0.8088\n",
            "Epoch 20/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4067 - accuracy: 0.8320\n",
            "Epoch 20: val_loss did not improve from 0.48034\n",
            "93/93 [==============================] - 23s 252ms/step - loss: 0.4067 - accuracy: 0.8320 - val_loss: 0.5177 - val_accuracy: 0.7892\n",
            "Epoch 21/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.8428\n",
            "Epoch 21: val_loss did not improve from 0.48034\n",
            "93/93 [==============================] - 23s 252ms/step - loss: 0.3876 - accuracy: 0.8428 - val_loss: 0.6851 - val_accuracy: 0.7843\n",
            "Epoch 22/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3946 - accuracy: 0.8387\n",
            "Epoch 22: val_loss did not improve from 0.48034\n",
            "93/93 [==============================] - 22s 242ms/step - loss: 0.3946 - accuracy: 0.8387 - val_loss: 0.5559 - val_accuracy: 0.7696\n",
            "Epoch 23/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.8455\n",
            "Epoch 23: val_loss did not improve from 0.48034\n",
            "93/93 [==============================] - 22s 239ms/step - loss: 0.3788 - accuracy: 0.8455 - val_loss: 0.6449 - val_accuracy: 0.7598\n",
            "Epoch 24/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3720 - accuracy: 0.8617\n",
            "Epoch 24: val_loss did not improve from 0.48034\n",
            "93/93 [==============================] - 23s 247ms/step - loss: 0.3720 - accuracy: 0.8617 - val_loss: 0.6977 - val_accuracy: 0.7451\n",
            "Epoch 25/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3887 - accuracy: 0.8374\n",
            "Epoch 25: val_loss did not improve from 0.48034\n",
            "93/93 [==============================] - 23s 252ms/step - loss: 0.3887 - accuracy: 0.8374 - val_loss: 0.4953 - val_accuracy: 0.7794\n",
            "Epoch 26/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.8516\n",
            "Epoch 26: val_loss did not improve from 0.48034\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.3788 - accuracy: 0.8516 - val_loss: 0.8105 - val_accuracy: 0.7745\n",
            "Epoch 27/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.8489\n",
            "Epoch 27: val_loss improved from 0.48034 to 0.47285, saving model to checkpoint_path.h5\n",
            "93/93 [==============================] - 30s 322ms/step - loss: 0.3841 - accuracy: 0.8489 - val_loss: 0.4729 - val_accuracy: 0.8235\n",
            "Epoch 28/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3582 - accuracy: 0.8536\n",
            "Epoch 28: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 22s 240ms/step - loss: 0.3582 - accuracy: 0.8536 - val_loss: 0.6533 - val_accuracy: 0.7696\n",
            "Epoch 29/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3686 - accuracy: 0.8563\n",
            "Epoch 29: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 22s 242ms/step - loss: 0.3686 - accuracy: 0.8563 - val_loss: 0.5389 - val_accuracy: 0.7843\n",
            "Epoch 30/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.8637\n",
            "Epoch 30: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 246ms/step - loss: 0.3501 - accuracy: 0.8637 - val_loss: 0.5360 - val_accuracy: 0.7843\n",
            "Epoch 31/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.8725\n",
            "Epoch 31: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.3246 - accuracy: 0.8725 - val_loss: 0.5698 - val_accuracy: 0.7304\n",
            "Epoch 32/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3513 - accuracy: 0.8610\n",
            "Epoch 32: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.3513 - accuracy: 0.8610 - val_loss: 0.7889 - val_accuracy: 0.7990\n",
            "Epoch 33/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.8684\n",
            "Epoch 33: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 250ms/step - loss: 0.3488 - accuracy: 0.8684 - val_loss: 0.6359 - val_accuracy: 0.7696\n",
            "Epoch 34/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8684\n",
            "Epoch 34: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 22s 241ms/step - loss: 0.3443 - accuracy: 0.8684 - val_loss: 0.6105 - val_accuracy: 0.8088\n",
            "Epoch 35/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.8704\n",
            "Epoch 35: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 249ms/step - loss: 0.3343 - accuracy: 0.8704 - val_loss: 0.5335 - val_accuracy: 0.7892\n",
            "Epoch 36/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.8806\n",
            "Epoch 36: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.3179 - accuracy: 0.8806 - val_loss: 0.5230 - val_accuracy: 0.8039\n",
            "Epoch 37/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.8785\n",
            "Epoch 37: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 243ms/step - loss: 0.3202 - accuracy: 0.8785 - val_loss: 0.5869 - val_accuracy: 0.7794\n",
            "Epoch 38/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.8752\n",
            "Epoch 38: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 244ms/step - loss: 0.3151 - accuracy: 0.8752 - val_loss: 0.6689 - val_accuracy: 0.7941\n",
            "Epoch 39/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.8907\n",
            "Epoch 39: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.2850 - accuracy: 0.8907 - val_loss: 0.5365 - val_accuracy: 0.7549\n",
            "Epoch 40/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8839\n",
            "Epoch 40: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 22s 238ms/step - loss: 0.3049 - accuracy: 0.8839 - val_loss: 0.4930 - val_accuracy: 0.7892\n",
            "Epoch 41/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3022 - accuracy: 0.8826\n",
            "Epoch 41: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 24s 258ms/step - loss: 0.3022 - accuracy: 0.8826 - val_loss: 0.5312 - val_accuracy: 0.7941\n",
            "Epoch 42/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.9022\n",
            "Epoch 42: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.2601 - accuracy: 0.9022 - val_loss: 0.6452 - val_accuracy: 0.8039\n",
            "Epoch 43/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9089\n",
            "Epoch 43: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 242ms/step - loss: 0.2576 - accuracy: 0.9089 - val_loss: 0.9000 - val_accuracy: 0.7353\n",
            "Epoch 44/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.8711\n",
            "Epoch 44: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 22s 241ms/step - loss: 0.3266 - accuracy: 0.8711 - val_loss: 0.5662 - val_accuracy: 0.7696\n",
            "Epoch 45/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9001\n",
            "Epoch 45: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 248ms/step - loss: 0.2509 - accuracy: 0.9001 - val_loss: 0.5914 - val_accuracy: 0.8186\n",
            "Epoch 46/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2846 - accuracy: 0.9001\n",
            "Epoch 46: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.2846 - accuracy: 0.9001 - val_loss: 0.6051 - val_accuracy: 0.7696\n",
            "Epoch 47/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9116\n",
            "Epoch 47: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 248ms/step - loss: 0.2550 - accuracy: 0.9116 - val_loss: 0.6349 - val_accuracy: 0.7794\n",
            "Epoch 48/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9150\n",
            "Epoch 48: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.2333 - accuracy: 0.9150 - val_loss: 0.8184 - val_accuracy: 0.7843\n",
            "Epoch 49/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9089\n",
            "Epoch 49: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 22s 237ms/step - loss: 0.2459 - accuracy: 0.9089 - val_loss: 0.7764 - val_accuracy: 0.7941\n",
            "Epoch 50/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.9028\n",
            "Epoch 50: val_loss did not improve from 0.47285\n",
            "93/93 [==============================] - 23s 245ms/step - loss: 0.2519 - accuracy: 0.9028 - val_loss: 0.6243 - val_accuracy: 0.7794\n",
            "7/7 [==============================] - 3s 421ms/step - loss: 0.5956 - accuracy: 0.7465\n",
            "Loss: 0.5956208109855652\n",
            "Accuracy: 0.7464788556098938\n",
            "Validation [0.6826650500297546, 0.6996835470199585, 2.046316623687744, 0.549851655960083, 0.49692994356155396, 0.5825701951980591, 0.5284667015075684, 0.5488384962081909, 0.5745561718940735, 0.6248442530632019, 240.3230743408203, 2.9243717193603516, 0.7532030940055847, 0.5980121493339539, 0.5729398131370544, 0.49535703659057617, 0.5231901407241821, 0.48034340143203735, 0.5132237672805786, 0.517723023891449, 0.6851339340209961, 0.5558930039405823, 0.6449159979820251, 0.6977002024650574, 0.4952525794506073, 0.8104555010795593, 0.4728529751300812, 0.6532707810401917, 0.5389490127563477, 0.5360113978385925, 0.5698085427284241, 0.7889329791069031, 0.6358718276023865, 0.6104860305786133, 0.5335047245025635, 0.5229739546775818, 0.5868566632270813, 0.6688812375068665, 0.5364896655082703, 0.49296993017196655, 0.5312120914459229, 0.6451621055603027, 0.8999968767166138, 0.5662405490875244, 0.5913730263710022, 0.6051233410835266, 0.6348655819892883, 0.8184036612510681, 0.7764419317245483, 0.6242716312408447]\n"
          ]
        }
      ],
      "source": [
        "# Compile o modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treine o modelo\n",
        "# r = model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=50, callbacks=[checkpoint], batch_size = 8)\n",
        "r = model.fit(dataset, validation_data=(X_val,y_val), epochs=50, callbacks=[checkpoint])\n",
        "\n",
        "# Avalie o modelo no conjunto de teste\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Exiba as métricas calculadas\n",
        "print(\"Loss:\", test_loss)\n",
        "print(\"Accuracy:\", test_accuracy)\n",
        "print(\"Validation\", r.history['val_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5OxTop9Xgp2"
      },
      "outputs": [],
      "source": [
        "loss = r.history['loss']\n",
        "val_loss = r.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxjnm_5vXiIV"
      },
      "outputs": [],
      "source": [
        "acc1 = r.history['accuracy']\n",
        "val_acc1 = r.history['val_accuracy']\n",
        "plt.plot(epochs, acc1, 'y', label='Training acurácia')\n",
        "plt.plot(epochs, val_acc1, 'r', label='Validation acurácia')\n",
        "plt.title('Training and validation acurácia')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('acurácia')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = np.load('datasets/X_test.npy')\n",
        "y_test = np.load('datasets/y_test.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "213\n"
          ]
        }
      ],
      "source": [
        "print(len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "model = tf.keras.applications.densenet.DenseNet121(\n",
        "    include_top=False,\n",
        "    weights=None,\n",
        "    input_shape=(256, 256, 3)\n",
        ")\n",
        "\n",
        "top = GlobalAveragePooling2D()(model.output)\n",
        "top = Dense(1024, activation='relu')(top)\n",
        "top = Dense(1, activation='sigmoid')(top)\n",
        "model = Model(inputs=model.input, outputs=top)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.applications.densenet import preprocess_input\n",
        "def preprocess_data(X, y, shuffle=False):\n",
        "    # Shuffle the data if specified\n",
        "    X = X/255\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(X))\n",
        "        np.random.shuffle(indices)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test, y_test = preprocess_data(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6667\n",
            "Precision: 0.6667\n",
            "Recall: 1.0000\n",
            "F1 Score: 0.8000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model.load_weights(os.path.join(pasta_do_notebook, 'weights', 'checkpoint_path.h5'))\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred_classes = np.round(y_pred)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes)\n",
        "recall = recall_score(y_test, y_pred_classes)\n",
        "f1 = f1_score(y_test, y_pred_classes)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
